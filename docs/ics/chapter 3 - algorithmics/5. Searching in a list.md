
To better illustrate time complexity, here's a search algorithm.

```python
def search(L, x):
	n = len(L)
	for i in range(n):
		if L[i] == x:
			return i
```

It's pretty good, as it simply checks every element, and if it's correct, returns it's index.
Doesn't exactly get much better than that, as:
if i is returned, we definitely know `L[i]` is x.
If i isn't returned, it's a bit more complex but it's doable to prove x is not in L.
And, lastly, if x in in the start of the list, it's $\Theta(1)$, but in the worst case, which is the important one, it's $\Theta$(n).


Now, here's a fun idea, what if we have a list, that is sorted?

Take a list that is 
`[-7, -5, -3, -1, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29]`

Well, we can look at the middle of the list. If it's smaller than x, then x is in the second half of the list. Otherwise, it's in the first half. And repeat.


```python
def binary_search(L, x):
	"""
	with a sorted L
	"""
	
	n = len(L)
	bottom = 0
	top = n-1
	
	while top >= bottom:
		middle = (bottom + top) // 2
		
		if L[middle] == x:
			return middle
			
		if L[middle] > x:
			top = middle - 1
			
		else:
			bottom = middle + 1
```

(if you want more details on how it works, add `print(L[bottom:top]))` in the while, before the ifs)

This is very interesting. We can fairly easily show it terminates.
If it returns something, it's trivial as it only answers something if `L[i]` is x.
However, to prove if it returns nothing, it's a bit more complex.
It can be done with a loop invariant, in this case 
> if x is in L, then x is in `L[bottom : top+1]`.

That is to say, if x is present, then it is in our interval.
Luckily, we eventually reach a state of that interval being $\emptyset$, which doesn't usually contain anything, which means it isn't in L.

As for proving that every iteration it holds true:
As L is sorted, if `L[middle]` is greater than x, then it can't be in `L[middle:top]`, as all these elements will always be greater than x. Same holds in the opposite direction as well. That means every element we remove can not be x. This means if x is present, it can't be removed. 
This proves this works.

Anyhow, what's the time complexity of this?
The answer is simple, in the worst case, we need to find how many times you need to divide the list by 2, until you get to 1. That means, it's in $\log_{2}(n)$, which we'll write as log(n)

What's very nice is that logarithms are extremely powerful, and they increase slower than x, and it's strictly positive.
It's also worth noting that $2^{k}\lt n\lt2^{k+1}$ means $\log(2^{k})\lt\log(x)\lt\log(2^{k+1})$
This is nice, as it allows us to simplify a bit of stuff. 

Don't tell the math teachers I wrote any of this.

Another nice thing, is that while a $(\log(n))^k$ might be greater than $(\log(n))^{k-1}$, for reasons, there is never a power at which log(n) is going to be greater than n or any of it's power.

For example, look at [this](https://www.desmos.com/calculator/frburrf0jh). Zoom out to about $10^{17}$ if necessary. 

Unfortunately, all of these nice benefits require a sorted list, which we'll get into next time.

It's also worth talking about exponential functions for a bit.

These are functions to the power of n. Anything to the power of n will always be greater than n to any power.

That's nice, because we'll get a new hierearchy:

- $\Theta$(1)
- $\Theta$($\log(n)$)
- $\Theta$($\log(n)^k$)
- $\Theta$(n)
- $\Theta$($n^{k}$)
- $\Theta$($2^{n}$)
- $\Theta$($2^{kn}$)

here's a little cheat sheet for this sort of stuff

![a lil cheat sheet](https://paper-attachments.dropbox.com/s_2D428973624E7FC84C7D69D11421DE762BEA6B6F3361231FCDCAE0425D14526F_1664885448372_Untitled.drawio+17.png)


[[4. Algorithmic complexity|Previous]]