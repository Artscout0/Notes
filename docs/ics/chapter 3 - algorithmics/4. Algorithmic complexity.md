
We've talked a bit about how correct various algorithms are, and proving they always work.

Now, we shall talk about performance.

When talking about it, we'll mostly focus on the time it takes to run, however it's worth noting for any future programmers that space in memory and number of memory calls is also decently important, but we'll talk about them way less.

## So, speed of algorithms eh?

As an algorithm is purely theoretical and independent of of the language, we can't exactly measure it in seconds.
Instead, we'll see how it scales with the largeness n of the input.

We'll mostly talk about algorithms that work with lists, because they work more intuitively.

Let's look at the following algorithm, written in Python

```python
def max_list(L):
	n = len(L)
	max = L[0]
	for i in range(1, n):
		if L[i] > max:
			max = L[i]
	return max
```

To measure the runtime, we can use the function `time()` from the `time` module

```python
from time import time

L = []
n = 10000
for i in range(n):
	L.append(random.randrange(1000))

t = time()
max_list(L)
print("run time:", time() - t)
```

However, that doesn't really help us.
We could probably plot how fast this runs on lists of 1, 2,..., 1000000 elements, and see how long it takes.

But what if we don't want to run the code 20 times, and don't exactly want to look or plot the results?
What if we want to do a bit of what of what we do in basically every other lesson and use some ✨math✨?


## A theoretical analysis of the parkour time of a function


Some operations we assume work in constant time:
- Arithmetic operations
- Boolean operations
- Data manipulation
- Flow control (if, loops...)
- Function calls
- Accessing the ith element of a list[^1]
- `len`, `append`, and `pop` list operations on a list

We also assume that the elements of the list we're working on aren't increasing in complexity.


So, let's analyse the `max_list()` function from before.

Everything from the start of the function until the for loop runs in constant time.
The return is also in constant time.

However, depending on the size of n, while the time of each iteration of the loop remains the same, the amount of iterations increases with n.

We thus know that depending on the size of the list, we get the function runs in linear time complexity.

## Comparing algorithms

Say we have a list, and we need to find the sum of it's two largest elements.
We can have multiple options:

```python
def max_sum(L):
	n = len(L):
	max_s = L[0] + L[1]
	
	for i in range(n):
		for j in range(i+1, n):
			if L[i] + L[j] > max_s:
				max_s = L[i] + L[j]
	
	return max_s
```


This algorithm brute forces it's way to the right answer. It iterates over every possible pair, and gets the best answer.

We can also prove it's correctness, using two ways.

> the two loops iterate over every distinct (i, j) pair for distinct i, j. At the start of the execution of the body, max_s contains the sum of the two largest previous i and j.

Or by having two loop invariant. The first option seems much easier to me.

Now, let's calculate:
the external loop executes n times
the internal loop executes it's body n - i - 1 times.

This allows us to calculate $t = C\cdot \frac{n(n-1)}{2}$ as the time of the loops. 
As we need to multiply n by n to get one of the result, we get that n is a quadratic function.

Another option is:

```python
def max_sum_two(L):
	n = len(L)
	max1 = max_list(L)
	L.remove(max1)
	max2 = max_list(L)
	return max1 + max2
```

So, we iterate over the list 2 times because of the `max_list()`, which we've explored a bit earlier, and an additional time because of the remove.

This means we still get a linear time.

It's clear that a linear time complexity will be better than a quadratic one, however it's still unclear exactly how to measure it. 

So, what we need  is a way to ignore the constant.

## Big O notation

We have created a thing for the exact purpose of measuring algorithmic complexity.

It's defined as follows:

Say there is $n\in\mathbb{N}$ and g, a positive function of n. O(g) is the set of the positive functions f(n) for which there exists reals C > 0 and N > 0 so that $\forall n \gt N, f(n)\le C\cdot g(n)$

An abuse of notation that everyone does and thus you can do to is to write instead of $f\in O(g)$, we write $\text{f is O(g)}$ or $f=O(g)$.

In human readable terms, starting from some point N, the time complexity of f is lesser than some constant times g(n).

A simple example is $1000n = O(n^2)$
Starting at roughly $N = 10000$, $1000n$ is lesser or equal  is lesser than $0.1n^2$.

Well, the example is boring.
What we can do instead is learn some simple rules this implies that make life easier:

$n=O(n), n=O(n^{2})$
$n^{2}=O(n^{3})$
$\sqrt{n}=O(n)$

$n^{p}=O(cn^{p})$ for any $c>0$.
If $p\lt q$, then $n^{p}=O(n^{q})$ however $n^{q}\ne O(n^{p})$.

$2n+100=O(n^{2})$
$n+10^{6}=O(n)$
$n^{2}+1000n+10^{6}=O(n^{3})$

It's also worth noting that O 'forgets' any constants inside it.

So, what we really need to do if find the lowest 'value' of O that works. 

## Big $\Omega$ notation

Similar to big O, it's also fairly poorly written.

Basically what we can say $\forall n \ge N f(n) \ge C \cdot g(n)$.

It's the set of all functions 'dominated' by g.


## Big $\Theta$ notation

It's the set of all function that both dominate and are dominated by g.

In other terms:

$\forall n > N, C_{1}\cdot g(n)\le f(n) \le C_{2}\cdot g(n)$

Basically, starting at some point, f(n) is between theta multiplied by two different constants.

An example would be $n^{2}+2n+100$.
$1.5n^{2}$ is bigger, $n^{2}$ is smaller, however they're both noted $\Theta(n^{2})$


This is what finally allows us to remove the ambiguity mentioned before, and what gives us a real way to compare theoretical algorithms on theoretical values.

It's worth noting this is far from perfect, as we're ignoring the actual contents of the data.
Something like a search algorithm that iterates over a loop will be $\Theta(1)$ if the first element we're looking for, while it'll be $\Theta(n)$ if it's at the end.

This is why we always assume the worst possible case.

If we have an $\Omega(n)$, this means we have a lower bound for how much time it'll take.
If we have an $O(n)$, we have a higher bound.
If we however have a $\Theta(n)$, this means have a lower and upper bound. 

As what we typically measure is the worst possible case, as mentioned before, the existence of an O(n) is better for us than the existence of $\Omega(n)$. This is also the reason we talk about it more often.
$\Theta(n)$ is very important to actually prove stuff however.

## TLDR

Essentially, we model how long an algorithm will take to run depending on the size of the input, and we write it down using O(n). We don't actually know how long it'll take, but it'll be proportional to the time the algorithm takes, multiplied by some constant.

We learned about O(n), which is essentially the worst case scenario.
We learned about $\Omega$(n), which is essentially the best case scenario.
We learned about $\Theta$(n), which is essentially equivalent to any scenario of this running.

A quick tip is that there's a hierarchy of complexity, from best to worse:

$\Theta$(1) - constant time
$\Theta$(n) - linear time
$\Theta$($n^2$) - exponential time
$\Theta$($n^{>2}$) - even more exponential time

Any algorithm in a lower theta in terms of runtime completely dominates anything in lower time complexity.
Our main goal is to find T(n), that is how much time it'll actually take. 
The most important thing to find is O(n), which is a complexity it'll never surpass.
Then we look at $\Omega$(n), which is an algorithm that never surpasses ours.
Lastly, we look at the intersection of O(n) and $\Omega$(n), which gives us something we can determine to be $\Theta$(n).

To actually find it, the best way is to look at the code. If there's a for loop, typically our answer is dependent on the nestedness of the loop. For example, for every nested loop, the time complexity increases by one power of n.

[[3. Recursive algorithms|Previous]]
[[5. Searching in a list|Next]]


[^1]:It's worth noting this one is somewhat unusual, it's the case in python, but it isn't necessarily the case in other languages like C
