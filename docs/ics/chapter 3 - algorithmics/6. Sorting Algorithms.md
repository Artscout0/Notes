
So, in the previous page we described how to search in a sorted list.

So, how do we sort a list?

There are as many answers as there are algorithms, and there are a lot of algorithms.

## Selection sort

The way this sort works is simple:

for every iteration, we look for the smallest element in the not yet sorted part of the list[^1], and swap it with the element at the number of the iteration.

A basic implementation:

```python
def selection_sort(L):
	n = len(L)
	for i in range(n):
		min_index = i
		for j in range(i+1, n):
			if L[min_index] > L[j]:
				min_index = j
		L[min_index], L[i] = L[i], L[min_index]
	# There is no return, as it updates L.
```

If course, we need to prove it works with a (well, 2) loop invariant(s)

The inner loop invariant is basically the same as for min/max.

As for the outer loop, we say that at the start of the iteration i, the sublist `L[0:i]` consists of the i smallest elements of L, that are sorted.

To prove it, well at the start i is 0, so `L[0:0]`, which is `[]`, which is accurate.

With each iteration, we find the smallest element, and put it at position i, which keeps the invariant true.

At termination, at `i = n`, we have the n first elements of the sublist (so all the elements), sorted and in order.

It's a nice and fairly intuitive algorithm, but it's time complexity isn't exactly perfect, as it's $\Theta(n^{2})$. So, not good.
Another of it's issues is that it doesn't have a best case worst case, it always takes $\theta(n^{2})$.

## Insertion sort

This is also a fairly simple algorithm:

At every iteration, it checks if the element at our new position i is smaller than the previous element. if it isn't, we move to the next. If it is, we swap it with the previous element until that isn't the case.

```python
def insertion_sort(L):
	n = len(L)
	
	for i in range(L):
		j = i
		
		while j > 0 and L[j] < L[j-1]:
			L[j], L[j-1]=L[j-1], L[j]
			j -= 1
```

It's got a nice invariant:
At the start of the iteration `i`, the sublist `L[0:i]` consists of the same elements as in the original `L[o:i]`, but sorted.

The worst case scenario is that the list is sorted in the opposite direction, it'll have a time complexity of $\Theta(n^{2})$.

## Are there better ways

Both are algorithms are $\Theta(n^{2})$. Can we do better?

Yes. Yes, by a loooot. 

We need to talk about a paradigm[^2], the **divide and conquer** paradigm. 

The algoryhm here is merge sort (rarely but sometimes called fusion sort.)

This algorithm breaks it's list in half, recursively, until we end up with 1 long lists.
If it receives a list of length 1, well a list of length 1 is sorted, so it's returned.

So, the algorithm receives then two lists of length 1, which it fuses to obtain a sorted list, which it returns.

It then has two 2 long sorted lists, which it merges together into a sorted list.

Here's my quick implementation of it:

```python
def merge(L1: list[int], L2: list[int]) -> list[int]:
	"""
	the part that merges 2 sorted lists
	"""
	res: list[int] = []
	# not not list returns True if list has items in it, so here it checks for 'not empty'
	while not not L1 and not not L2:
		if L1[0] <= L2[0]:
			# pop removes the element, append adds it
			res.append(L1.pop(0))
		else:
			# pop removes the element, append adds it
			res.append(L2.pop(0))
	
	while not not L1:
		res.append(L1.pop(0))
	while not not L2:
		res.append(L2.pop(0))

	return res

  

def merge_sort(L: list[int]) -> list[int]:
	"""
	the part that creates the 2 sorted lists.
	"""
	print(L)
	n = len(L)
	
	if n <= 1:
		return L

	L1 = merge_sort(L[0:n//2])
	L2 = merge_sort(L[n//2:n])

	return merge(L1, L2)
```

Technically it isn't the best possible implementation, as it creates new lists every time, and thus you need to return it and assign it, but I don't really care.

To prove the correctness of merge_sort is easy, it's correct if a list of size 1 is provided, and if merge is correct. Merge can be proven correct using a loop invariant.

To show it's time complexity is a bit annoying.
We can show that merge() is $\Theta(n)$. We're going to assume, and we're not allowed to do this but who cares, that it's actually $\Theta(cn)$, where c is a constant.

The time that merge_sort takes is a bit more complex:
$T(n)=2T(\frac{n}{2})+M(n)$
So: $T(n)=2T(\frac{n}{2})+cn$.

So... How do we find it? Well, we'll assume that n is a power of two, because it makes life easier and is often the case.

So, let's do some math.

At each step, the constant time is always cn. At each step, the further down we go, the closer the local n gets to 0, and eventually we arrive to c + 2T(1) (as eventually the local n is 1). So we end up with as many constant times as is needed to reach the bottom of the 'tree', times T(1)\*n, as that's the amount of times we need to run merge sort of lists of length 1.

We thus end up with $\Theta(n\log(n))$[^3]

It's a bit slower than just $\Theta(n)$, but it's much slower than any power of n. It's decent enough, and is more than fast enough for 90% of algorithms.

Technically we weren't exactly thorough, we didn't prove the cn part, but good enough.

Also look up the master theorem, it's quite important.

## Other sorting algorithms not mentioned in the lesson

As I've mentioned before, there's tons of sorting algorithms. Most are useless, some are fun, the rest are decent.

Some I like are the following ones:

Stalin sort. It's technically $\Theta(n)$, it only loops through the list once, and if an element is not in order, it is purged (removed) from the list.

Bogosort. It's technically $\Theta(1)$. Or $\Theta(\infty)$. Depends. What it does, is it rearranges the lit randomly, and checks if it's sorted. The time it'll take depends directly on the amount of times it'll randomly rearrange the values wrong.

Quantum Bogosort. It's technically $\Theta(1)$. It randomly rearranges the list, but it assumes the Many-Worlds interpretation is correct, and thus only works in the word where quantum bogosort always returns the correct list immediately. 

Heatdeath Sort. It's $\Theta(whatever)$. It doesn't really do much, it's a simple sorting algorithm, however it has a constant time of roughly until the heat death of the universe, to highlight the mild absurdity of ignoring the constant and assuming it'll always be dwarfed by n and it's powers.


[[5. Searching in a list|Previous]]
[[7. Graphs|Next]]

[^1]:If I accidentally write "array" instead of list, just assume it's the same thing. It isn't, but for all intents and purposes for beginner level programming, which is what we're doing here, they're the same thing.

[^2]:Fancy word people use when they mean "a concept in programming"

[^3]:log being still in base 2
