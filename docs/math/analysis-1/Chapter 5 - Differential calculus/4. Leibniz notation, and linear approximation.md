
>[!note]- historical note
>Gottfried Wilhelm Leibniz was a guy who lived a while back, and did a whole bunch of science. Like a lot.
>His main thing is that he, independently from Newton, invented calculus (what we're doing here).
>He also did a whole bunch of philosophy, bit of metaphysics, found a [[3 - geometric series|series]] for pi, more or less started the field of topology, and a whole bunch of physics.
>I hate him less than Newton.


So, as we've learned, a guy named Lagrange decided to write 'derivative of a function' using f'. It's easy, nice, convenient, but fairly unintuitive.

We also have learned in physics of the notation $\dot{x}$, as the derivative of x, and that's what Newton did.

Well, Leibniz also had his own notation, which is probably the most used one of the 3.

You might remember earlier the notation $\lim_{\Delta x\to0} \frac{\Delta f}{\Delta x}$.
Well, Leibniz being a mathematician (thus by definition lazy), simplified it as:

$$
f'(x_{0})= \frac{dy}{dx}(x_{0})= \frac{df}{dx}(x_{0})=\frac{d}{dx}y(x_{0})=(\frac{d}{dx}f)(x_{0})=\dots
$$

(there's as many ways to denote this as there are ways to do make raclette, and every Swiss person has their own "only correct way" to do raclette).

dx essentially means $\Delta x$ with $\Delta x$ very very tiny, which we call an infinitesimal x.

The notation can allow to write stuff that'll horrify any math teacher (and gain you approval from physics ones)

$$
\frac{df}{dx}=f'(x)\iff df=f'(x)dx
$$

Which... We'll learn why that works later (as in bachelors levels of later).

## Linear approximation

We've previously learned that $f'(x_{0})=\lim_{\Delta x\to0}= \frac{f(x_{0}+\Delta x)- f(x_{0})}{\Delta x}$.

Let's note $r(\Delta x)$, which is $\frac{f(x_{0}\Delta x)- f(x_{0})}{\Delta x}-f'(x_{0})$. Of course, this means $\lim_{\Delta x\to0}r(\Delta x)0$.

We thus have $\Delta x r(\Delta x)= f(x_{0}+\Delta x)- f(x_{0})$.

This can be turned into $f(x_{0}+\Delta x)= f(x_{0})+f'(x_{0})\Delta x+ \Delta x r(\Delta x)$, with same limit.

Which, if you look it, looks a whole lot like the equation of the tangent.. + something.
r is named that way because it's the remainder. What's preventing the tangent from being the function.

And this, means that, because r is so extremely tiny, we can, in the same cases we'd use $\pi=3$ and $g=10$, that that tangent is that function.

In other terms, if $\Delta x$ is very tiny, $\Delta x r(\Delta x)$ is very very small, and so $f(x_{0})+f'(x_{0})\Delta x$approximates $f(x_{0}+\Delta x)$, denoted $f(x_{0})- f'(x_{0})\Delta x\approxeq f(x_{0}+\Delta x)$.

Which is nice because $f(x)$ can be very complicated, and computers aren't that good at many things, but they are really really good at additions and multiplications[^1], which are the core components of any line.

## So, what actually is linear approximation


If we have a function f, derivable at $x_{0}$, the linear approximation of $f$, at $x_{0}$ for a small $\Delta x$ we call the linear approximation.

$$A=f(x_{0})+f'(x_{0})\Delta x$$

So, some notes:

If we write $\Delta x=x-x_{0}$:
$f(x_{0}+\Delta x)\approxeq f(x_{0})+f'(x_{0})\Delta x \iff f(x)\approxeq f'(x_{0})+f'(x_{0})(x-x_{0})$.


Another note is the link with Leibniz's notation:
$f(x_{0}+\Delta x)=\approxeq f(x_{0})+f'(x_{0})\Delta x\iff f(x_{0}+ \Delta x)-f(x_{0})\approxeq f'(x_{0})\Delta x$

Which means two things, first of all, the best approximation at the limit is that tangent.
Another thing is

$$\frac{f(x_{0}+\Delta x)- f(x_{0})}{\Delta x}\approxeq f'(x_{0})$$

## Relation to infinitely small equivalents

$f(x_{0}+\Delta x)= f(x_{0})+f'(x_{0})\Delta x+ \Delta xr(\Delta x)$
Replacing delta x
$f(x)= f(x_{0})+f'(x_{0})(x-x_{0})+(x-x_{0})r(x-x_{0})$
$\iff f(x)-f(x_{0})=f'(x_{0})(x-x_{0})+(x-x_{0})r(x-x_{0})$
If we now assume f'(0) is not 0:

$$\frac{f(x)-f(x_{0})}{f'(x)}= 1+ \frac{r(x-x_{0})}{f(x_{0})}$$
Which means that the functions $f(x)-f(x_{0})$ an $f'(x_{0})(x-x_{0})$ are infinitely small equivalents to each other in the neighborhood of $x_{0}$.

A very good example is $f(x)=\sin(x)$, and $x_{0}=0$.
That is because:

$$\frac{f(x)-f(x_{0})}{f'(x)(x-x_{0})}=\frac{\sin(x)}{x}$$

This means x is the best approximation of $\sin(x)$.

And this is one of the things that leads to Taylor[^2] series.

## Examples because this is confusing

### Example 1

$\sin(x)\approxeq x$ and $x_{0}=0$

if we take sin(1) ~ 1, sin(x) is approximately 0.8414, which is a loss of 15%.
If we take sin(0.5) ~ 0.5, sin(x) is 0.4794
sin(0.1) ~ 0.1, while the actual value is 0.0998
sin(0.001) ~ 0.001, is 0.000999

### Example 2

Let's approximate $\sqrt[3]{8.012}$. If we approximate with $f(x_{0}+\Delta x)$, with you guessed it $f(x)=\sqrt[3](x)$, $x_{0}=8,\Delta x=0.012$

If we evaluate $f(x_{0}+\Delta x)\approxeq f(x_{0})+f'(x)\Delta x$
So $\sqrt[3]{8.012}\approxeq \sqrt[3]{8}+f'(x)\cdot0.012$
The derivative of the cubic root is easy to find, it's $\frac{1}{3}x^{\frac{2}{3}}$. If we plug in x = 8: $\frac{1}{12}$.

This allows us to find $2+ \frac{1}{12}\cdot0.012$.

which is roughly 2.001

The answer the calculator would give is $2.0009994$.
So, not too bad I'd say.

[[3. Continuously differentiable function|Previous]]

[^1]:To understand why, I suggest checking out https://www.nandgame.com/

[^2]:Another mathematician mentioned
