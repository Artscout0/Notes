
Say we have f, applied linearly from R2 to R2, as well as A, which is the relevant matrix of f.

Our goal is to 'reduce f', that is to say turn f into a simpler (simplest) form.

We previously solved this problem for matrices one can diagonalize, but we don't have a general way to do it for.

## Case where the $\Delta$ is 0, and f isn't an identity matrix

This means we can transform the matrix into $\chi_{f}(x)=(x-\omega)^{2}$, but f isn't diagonalizable.

This will result in us having effectively a single eigenvector. 

So, we suppose there exist bases B of R2, that follow the following form:

$$
[f]_{B}=\begin{bmatrix} \omega&1\\0&\omega \end{bmatrix}
$$


This has the properties of:

It's the $P^{-1}AP$ of this matrix 
It represents $\begin{cases}t'_{1}=\omega t_{1}+t_{2}\\ t'_{2}=\omega t_{2}\end{cases}$

We'll later find that we can use any vector as v2, as long as it isn't the same as the kernel of $f-\omega I_{\mathbb{R}^{2}}$.

Let's note that on $[f]_{v_{2},v_{1}}$, we simply invert the position of the 1 $\begin{bmatrix} \omega&0\\1&\omega \end{bmatrix}$.

These are the two reduced forms of f (or A).


### Example

Let's say we have $f: (x,y)\to(5x+3y,-3x-y)$, equivalent to $A=\begin{bmatrix}5&3\\-3&-1\end{bmatrix}$.

We can find 2 to be an eigenvalue of this matrix (using det(A - kI)=0)

$\ker(f-2I_{2})$ is $x+y=0$, so we can take v2  to be $v_{2}=(1,0)$.

This means because of $f(v_{2})=v_{1}+2v_{2}$, obtained from $\begin{bmatrix}2&1\\0&2\end{bmatrix}$, we can determine v1 for that to be 

$$
f(v_{2})-2v_{2}=(5,-3)-(2,0)=(3,-3)
$$

Which is an eigenvector.

This allows us to say that $B_{v_{1},v_{2}}$, with $v_{1}=(3,-3),~v_{2}=(1,0)$, places f into reduced form.

## If the $\Delta$ is less than 0

A good example of these matrices is rotation matrices.
More specifically, we can show any matrix like this can be turned into a variation of a rotation matrix.

In this case, we can write

$$\chi_{f}(x)=(x-\omega)^{2}+\epsilon$$

We don't have real roots, however in the complex plane, which we haven't learned about yet, it's roots are $\omega\pm i\epsilon$.

### Example

$f:(x,y)\to(3x+4y,-2x-y)$

$A=\begin{bmatrix}3&4\\-2&-1\end{bmatrix}$

the trace can be found to be 2, the determinant 5. 

If we complete the square:

$$x^{2}-2x+5=(x-1)^{2}+2^{2}$$

So, the idea is that there exists a basis B of R2, with:

$[f]_{B}=\begin{bmatrix}\omega&-\epsilon\\\epsilon&\omega\end{bmatrix}$

I'm not writing the proof, it's basically the same as the one before, we find v1 after setting v2, or in the opposite direction.
In any case, we end up with $v_{2}= \frac{1}{\epsilon}(f(v_{1})-v_{1})$

Let's note that if $[f]_{v1,v2}=\begin{bmatrix}\omega&-\epsilon\\\epsilon&\omega\end{bmatrix}$, then $[f]_{v_{2},v_{1}}=\begin{bmatrix}\omega&\epsilon\\-\epsilon&\omega\end{bmatrix}$.

These two matrices are called the reduced forms of f (or A).


So, with that, the original matrix isn't symmetrical, but the result's diagonals are nice and easy to use.

If we go back to our example, we can find $[f]_{B}=\begin{bmatrix}1&-2\\2&1\end{bmatrix}$.

## Proof we can use any vector

We base ourselves on $A^{2}-tr(A)(A)+\det(A)I_{2}=0$, which is a theorem.

In the case of $\Delta=0$, we have $\chi_{f}(x)=x^{2}-2\omega+\omega^{2}$, equivalent to $(x-\omega)^{2}$.

This implies $A^{2}-2\omega A+ \omega^{2}I_{2}=0$, which is equivalent to $(A-\omega I_{2})^{2}$

We thus get:

$$(f- \omega I_{2})(v_{1})= (f-I_{2})\circ(f-I_{2})(v_{2})$$

Which means $f(v_{1})=\omega v_{1}$, as $(A-\omega I_{2})^{2}=0$, meaning $v_{1}$ is an eigenvector of f. In the base v1,v2, we have:

$$[f]_{B}=\begin{bmatrix}\omega&1\\0&\omega\end{bmatrix}$$
The same logic with a bit more maths applies to the case where $\Delta<0$.

[[1. Diagonalizing matrices|Previous]]