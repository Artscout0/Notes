
So, after too many pages of set theory, we finally get to matrices.

A matrix is similar to a vector, at it's an ordered list of numbers.
It's different, as it'd 2 dimensional, and looks something like this:

$$
\begin{bmatrix} a_{1,1} & a_{1,2} & \dots & a_{1,n} \\ a_{2,1} & a_{2,2} & \dots & a_{2,n}  \\ \vdots &\vdots & \ddots & \vdots \\ a_{m,1} & a_{m,2} & \dots & a_{m,n} \end{bmatrix} = [a_{i,j}]_{\begin{matrix} 1\le i\le m \\1 \le j \le n\end{matrix}} \in M_{m,n}(\mathbb{R})
$$

A matrix can only have one line, it's a line matrix.
A matrix can have only a column, which is a column matrix.

If you have a line and a colum matrix, with same length and same values, it's still not the same matrix.

If a matrix has the same height and length, it's called a square matrix.


Some matrices also have other special names.
For example you can have a null matrix, which is a matrix of size m,n that only contains 0s.

There are also matrices called identity matrices, the common ones are:
$I_{2}=\begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix}$ and $I_{3}=\begin{bmatrix}1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0  & 1\end{bmatrix}$
(you might also hear of pseudo identity matrices, but that doesn't really matter most of the time, it's just Identity matrices that aren't square.)

## Matrix operations

The matrix operations that you need to know are:

Scalar multiplication
Addition
Product (regular multiplication)
Transposition

### Scalar multiplication

Much like with vectors, it multiplies every value in the matrix by a scalar.

$2\begin{bmatrix} 1 & 2 \\ -3 & 4\end{bmatrix}=\begin{bmatrix} 2 & 4 \\ -6 & 8\end{bmatrix}$

## Matrix addition

For two same sized matrices, you add their individual values at their various indexes together.

$$\begin{bmatrix} 1 & 2 \\ -3 & 4\end{bmatrix}+\begin{bmatrix} 2 & 4 \\ -6 & 8\end{bmatrix}=\begin{bmatrix} 3 & 6 \\ -9 & 12\end{bmatrix}$$

For different sized matrices, you can't do that.

## Transposition

You basically rotate the matrix.

You flip the values around the center line (where m and n are equal).

$$t\begin{bmatrix} 1 & 2 \\ -3 & 4\end{bmatrix}=\begin{bmatrix} 1 & -3 \\ 2 & 4\end{bmatrix}$$

## Product

It's more complex.

It's basically taking the first line of the front matrix, and multiplying it with the 1st column of the second matrix, then you go to the next column, then next line, and... It's complicated to explain.


Basically:

$$
\begin{bmatrix} 2 & 3 \\ 4 & -1 \end{bmatrix} \times \begin{bmatrix}1 & 5  \\ -3 & 4 \end{bmatrix}=\begin{bmatrix}(2, 3)\binom{1}{-3}=-7 & (2, 3)\binom{5}{4}=31 \\ (4, -1)\binom{1}{-3}=7&(4, -1)\binom{5}{4}=13\end{bmatrix}
$$

It's worth noting this is not commutative.

It's also worth noting this works only on matrices with where the width of the first matches the height of the second one.

Multiplying a column by a line matrix results in an interesting thing.

Next, another thing you can do is:

$$\begin{bmatrix} 2 & 3 \\ 4 & -1 \end{bmatrix} \times \begin{bmatrix}x  \\ y \end{bmatrix}=\begin{bmatrix}2x+3y\\4x-y\end{bmatrix}$$

And another thing:

$$
\begin{bmatrix}x  \\ y \end{bmatrix} \times \begin{bmatrix} 2 & 3 \\ 4 & -1 \end{bmatrix} = Undefined
$$

And lastly:

$A = \begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix}$

$$A \times I_{2}= \begin{bmatrix}\alpha & \beta \\ \gamma & \delta\end{bmatrix}$$

In other terms, multiplying by the identity matrix is the same as multiplying by one.

> [!Note]- For the programmers out there
> It can be noted that it's fairly easy to compute, assuming a matrix represented by a 2d list (so a list of lists)
> quick implementations in python:
> 
> ```python
> matrix1 = [[1, 2], [3, 4]]
>matrix2 = [[5, 6], [7, 8]]
>
>def matrix_product(matrix1, matrix2):
>	
>   # Check that the matrices can actully be multiplied
>   if len(matrix1[0]) != len(matrix2):
>        raise Exception("undefined")
>  
>   # Transpose matrix2 to easily get its columns
>   matrix2_cols = list(zip(*matrix2))
>
>   # Compute the product
>   result = []
>   for i in range(len(matrix1)):
>       row_result = []
>       for col in matrix2_cols:
>    
>       # Dot product of matrix1[i] and col
>       val = sum(a * b for a, b in zip(matrix1[i], col))
>       row_result.append(val)
>       result.append(row_result)
>
>return result
> ```

## Numeric variants (2x2)

There are numbers that a matrix has, that are arguably more important that it's coefficients.

For a matrix $A=\begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix}$

If we look at a matrix like a collection of vectors, you can get the determinant.

It's found using: $det A=\begin{vmatrix} \alpha & \beta \\ \gamma & \delta \end{vmatrix}=\alpha\delta-\beta\gamma$

You can also get it's rank, that is in how many different directions they are pointing, for a 2x2 matrix there are 3 options:

The null matrix, which doesn't point anywhere and has a rank of 0.

In cases where the first and second column of the matrix are colinear, so $\exists k \in \mathbb{R}, k\binom{\alpha}{\gamma}=\binom{\beta}{\delta}$, they are pointing along a single line (so by using a combination of both, you can point at any point in 1D space). That means it has a rank of 1.

In any other case, using a combination of the two vectors you can point to any point in 2D space, it has a rank of 2.

Examples:

$A=\begin{bmatrix}-1 & 2  \\ 1 &-6\end{bmatrix}$
det A = 10
tr A = -8
rg A = 2

$A=\begin{bmatrix}2 & 4  \\ 3 & 6\end{bmatrix}$

det A = 0
tr A = 8
rg = 1

this shows us we can build a rank 1 matrix it using a single line and column, we call them column-line decompositions.

## Matrix inversion

Well, an inverse matrix is simply the matrix, multiplied by which, a matrix becomes an identity matrix.

Written $A^{-1}$.

The way we usually find it is:

$A^{-1}=\frac{1}{\alpha\delta- \beta \gamma}\begin{bmatrix}\delta & -\beta \\ -\gamma & \alpha\end{bmatrix}$

It's worth noting this doesn't always work, it only really applies to rank 2 matrices. Rank 1 have multiple options, and rank 0... well anything \* 0 is 0, so it's not really possible.

So, if we do this:

$$\frac{1}{\alpha\delta- \beta \gamma}\begin{bmatrix}\delta & -\beta \\ -\gamma & \alpha\end{bmatrix}\times \begin{bmatrix}\alpha & \beta \\ \gamma & \delta\end{bmatrix}=\frac{1}{\alpha\delta- \beta \gamma}\begin{bmatrix}\alpha\delta-\beta\gamma & 0 \\0 & -\beta\gamma+\alpha\delta \end{bmatrix}=\begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix}$$


It's also worth saying that, again, that works with rank 2 matrices. Rank 0 doesn't work as it isn't reversible.


Some last minute rules:

$A + B = B + A$
$AB\ne BA$
$(A+B)C = AC + BC$ ($\ne CA+CB$)
$(A+B)^{2}=A^{2}+AB+BA+B^{2}$
