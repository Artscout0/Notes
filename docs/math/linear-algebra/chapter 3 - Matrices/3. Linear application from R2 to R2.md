
## Definition

A linear application is some application $f:R^{2}\to R^{2}$  is linear if there exists $\alpha,\beta,\gamma,\delta\in\mathbb{R}$, so that:

$\forall v\in\mathbb{R}^{2}:f(v)=(\alpha x+ \beta y, \gamma x + \delta y)$

We thus have:

$[f(v)]_{B_{com}}=\binom{\alpha x+\beta y}{\gamma x + \delta y}=\begin{bmatrix}\alpha&\beta\\\gamma&\delta\end{bmatrix}\begin{bmatrix}x\\ y\end{bmatrix}=A[v]_{B_{can}}$

Where A is a matrix of f in the cannonical base.

Basically, a linear application is a multiplication by a matrix.

## Examples:

a.
$\begin{bmatrix}0&0\\0&0\end{bmatrix}, (x,y)\to(0,0)$
(this turns it to 0)

b.
$I_2=\begin{bmatrix}1&0\\0&1\end{bmatrix}, (x,y)\to(x,y)$
(this is identical to the original)

c.
$2I_{2}=\begin{bmatrix}2&0\\0&2\end{bmatrix}, (x,y)\to(2x,2y)$
(this is a [[6. Translations and homotheties#Homothety|homothety]])

d. 
$\begin{bmatrix}1&0\\0&0\end{bmatrix}, (x,y)\to(x,0)$
(this is a projection)

(all these examples are easy, as they contain a whole lot of zeros...)

This has the fun meaning that we'll have matrices for every type of transformation from analytic geometry.

## Characterization of linear application

Suggestion: $f:\mathbb{R}^{2}\to\mathbb{R}^{2}$ is linear only if:

$$
\begin{matrix} \forall v\in\mathbb{R}^{2},\forall t\in\mathbb{R} && \forall v,v'\in\mathbb{R}^{2}\\ f(tv)=tf(v)&&f(v+v')=f(v)+f(v') \end{matrix}
$$


## Example study

### quick look at some examples

Say we have $f: \mathbb{R}^{2}\to\mathbb{R}^{2}, (x,y)\to(2x-y,x+y)$
This can be described as $\begin{bmatrix}2&-1\\1&1\end{bmatrix}$.

We can solve this as

$$
\begin{cases}2x-y=a\\x+y=b\end{cases}\iff\begin{cases}x=\frac{a+b}{3}\\ y=\frac{-a+2b}{3}\end{cases}
$$

(we find this by finding $A^{-1}$, that is to say $\frac{1}{3}\begin{bmatrix}1&1\\-1&2\end{bmatrix}$)
It describes a rotation and a homotheties
f is also bijective.


Say we now have $f:\mathbb{R}^{2}\to\mathbb{R}^{2}, (x,y)\to(3x-3y, x-y)$
which is described by $\begin{bmatrix}3&-3\\1&-1\end{bmatrix}$, which can be simplified as $\begin{bmatrix}3\\1\end{bmatrix}\times\begin{bmatrix}1&-1\end{bmatrix}$

It describes a projection.
It's not bijective, as it's neither injective nor surjective.

It's worth noting that $f^{-1}(\{(a,b)\})=\begin{cases}a\ne3b\implies\emptyset \\ a=3b\implies x-y=b\end{cases}$

Essentially that means any values we have get turned into lines, which are themselves projected onto another line. Or something along these lines, visualize this in desmos or something. 

### Kernel of the matrix

Let's look back at the rotations from before.
We note:

$Im f=f(\mathbb{R}^{2})$, which is the image of f, and $\ker f=f^{-1}(\{(0,0)\})$

If we use arguments similar to some other earlier ones...

if the matrix's rank (rg A) is 0,It's image is $(0,0)$, and it's $\ker f = r^2$
If $rgA=1$, it's image is $Vect((\lambda, \mu))$ $\ker f=ax+by=0$
If $rgA=2$, it's image is $\mathbb{R}^{2}$, and it's kernel $\ker \{(0,0)\}$

In summary, and this a summary exclusive to $\mathbb{R}^{2}$, $Imf$ and $\ker f$ are vector subsets of $\mathbb{R}^{2}$, and we always have:

$$\dim(Im f)+\dim(\ker f)=2$$

This is (a simplified version of) the rank theorem.

## How to actually represent matrices

Let's use the earlier notations again, so $A=\begin{bmatrix}\alpha&\beta\\\gamma&\delta\end{bmatrix}$ and $f: (x,y)=(\alpha x+\beta y, \gamma x + \delta y)$

Now, we can break down any 2d matrix into two vectors, here $\binom{\alpha}{\gamma}$ and $\binom{\beta}{\delta}$, and if we remember the idea of a Base, we can remember a base is also defined using 2 vectors.

**Definition**: We call the matrix of f in base B the matrix:

$[f]_{B}=P^{-1}AP=\begin{bmatrix}\lambda&\mu\\ \rho&\sigma\end{bmatrix}$

Essentially, the idea is to turn as many elements from A into 0s as possible wihout changing any values.

Suggestion: FFor any $v\in \mathbb{R}^{2}$, we have:

$[f(v)]_{B}=[f]_{B}[v]_{B}$

The proof: Let's write v as (x,y).

$[f(v)]_{B}=P^{-1}[f(v)]_{B_{can}}=P^{-1}A[v]_{B_{can}}$

which is $P^{-1}A\binom{x}{y}= P^{-1}APP^{-1}\binom{x}{y}$

which is the same is $P^{-1}AP[v]_{B}$


Which can be boiled down to:

for a base $B=\begin{bmatrix}\lambda&\mu\\ \rho& \sigma\end{bmatrix}$

$[f(v_{1})]_{B}=\binom{\lambda}{\rho}$ and $[f(v_{2})]_{B}=\binom{\mu}{\sigma}$

That isn't to say $f(v_{1}) = (\lambda, \rho)$, but that $f(v_{1})= \lambda v_{1} + \rho v_{2}$

### Examples:

$f: (x,y)\to\frac{1}{7}(6x+2y,3x+y)$

This is $\frac{1}{7}\begin{bmatrix}6&2\\3&1\end{bmatrix}$.
$B=B_{can}=v_{1} (1, 0), v_{2}(0, 1)$ which means P = $\begin{bmatrix}1&0\\0&1\end{bmatrix}$.

this means 
- $[f]B_{can}=P^{-1}AP=I_{2}^{-1}AP=A$
- $[f(v)]_{B_{can}}= \frac{1}{7}\begin{bmatrix}6x+2y \\ 3x + 1y\end{bmatrix}= \frac{1}{7}\begin{bmatrix}6&2\\3&1\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}=A[v]_{B_{can}}$


next: 

$f:\frac{1}{7}(6x+2y,3x+y)$
which gets us $A=\begin{bmatrix}6&2\\3&1\end{bmatrix}$
$B=v_{1},v_{2}$ where v1=(2,1) and v2=(-1,3)

if we apply $P^{-1}AP$, we get $[f]_{B}=\begin{bmatrix}1&0\\0&0\end{bmatrix}$ 
for any v, we get $[v]_{B}=\binom{t_{1}}{t_{2}}\implies[f(v)]_{B}=\begin{bmatrix}t_{1}\\0\end{bmatrix}$
We see that f is projected onto v1, which is parallelly to v2 

Lastly, f(v1) is equal to v1, while f(v2) is (0,0)

## Numerical Invariants

for any base B of $\mathbb{R}^{2}$, we have:

$\det [f]_{B}=\det A$, $tr [f]_{B}=tr A$, and $rg [f]_{B}=rg A$

The proofs are:

$\det [f]_{B}=\det(P^{-1}AP)=\det{APP^{-1}}=\det(AI_{2})=\det{A}$

The same procedure works for the trace and the rank.

[[2. R2 Vector Space|Previous]]
