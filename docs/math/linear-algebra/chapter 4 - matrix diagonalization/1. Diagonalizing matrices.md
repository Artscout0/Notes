i

So, there are many cases where you want to turn a standard matrix with every part filled with various numbers into something that looks like

$$
\begin{bmatrix}\mu&0\\0&\nu\end{bmatrix}
$$

That is equivalent if we evaluate it.

So, let's define some stuff for later:

$f:\mathbb{R}^{2}\to\mathbb{R}^{2}, ~(x, y)\to(\alpha x+\beta y, \gamma x+ \delta y)$
That is represented using the matrix
$A=\begin{bmatrix}\alpha&\beta\\ \gamma&\delta\end{bmatrix}$



## Definition of the Characteristic Polynomial

The characteristic polynomial of f (or A) is defined by  

$$\chi_f(x) = \det(A - xI_2)$$

For a (2 by 2) matrix (A), this expands to  


$$\chi_{f}(x) = x^{2} - (\operatorname{tr} A) x + \det(A)$$


It's eventual roots are called the eigenvalues of f (or A).

>[!note]+ For the french terminology
> eigenvalues is the term for "valeurs propres", and everything with "propre" in it's name is replaced with some flavor of "eigensomethingorother".
> Specifically in this document:
>  "valeurs propres" - "eigenvalues"
>  "vecteurs propres" - "eigenvectors"
>  "sous-ensembles propres" - "eigenspaces"


So, let's say we have $\omega\in\mathbb{R}$ and $v\ne(0,0)$. We have:

$f(v)=\omega v\iff v\in \ker(f-\omega id_\mathbb{R})$

In this case, we say that v is the eigenvector of f for the eigenvalue $\omega$.

The proof is that we have $f(v)=\omega v\iff v\in \ker(f-\omega id_\mathbb{R})$, as well as $\ker(f-\omega I)\ne\{0\}\iff \chi_{f}(\omega)=0$

The set $\ker(f-\omega id_{\mathbb{R}^{2}})$ is called the eigenspace of f.

And that's everything we really need. This is nice because for now we're stuck in 2D, it'll get more complicated over time.

## Examples

### Ex 1

Say we have:

$f:(x,y)\to(\frac{5x+9y}{4}, \frac{3x-y}{4})$, which has $A=\begin{bmatrix} \frac{5}{4}& \frac{9}{4}\\ \frac{3}{4}& \frac{-1}{4}\end{bmatrix}= \frac{1}{4}\begin{bmatrix}5&9\\3&4\end{bmatrix}$

From there, we can find:

$$
\begin{cases}tr(A)= \frac{1}{4}(5-1)=1\\
\det(A)= \frac{1}{16}(-5-27)=-2\end{cases} \implies
\chi_{f}(x)= x^{2}-x-2 =(x+1)(x-2)
$$

f thus has 2 eigenvalues, -1, and 2.

with $\omega$ = -1:

$$
A+I_{2}= \frac{1}{4}\begin{bmatrix}5&9\\3&-1\end{bmatrix}+ \frac{1}{4}\begin{bmatrix}4&0\\0&4\end{bmatrix}= \frac{1}{4}\begin{bmatrix}9&9\\3&3\end{bmatrix}= \frac{1}{4}\begin{bmatrix}9\\3\end{bmatrix}\begin{bmatrix}1&1\end{bmatrix}
$$

Which means has a kernel of $x-y=0$.

In the case of $\omega=2$:

$A-2I_{2}=\dots= \frac{1}{4}\begin{bmatrix}-3&9\\3&-9\end{bmatrix}= \frac{1}{4}\begin{bmatrix}-3\\3\end{bmatrix}\begin{bmatrix}1&-3\end{bmatrix}$

Which has a kernel of $x-3y=0$

### Ex 2

$f:(x,y)\to(3x,3y)$, which has an A of $\begin{bmatrix}3&0\\0&3\end{bmatrix}$

3 is the only eigenvalue, and $\ker(f-3id_{\mathbb{R}^{2}})=\mathbb{R}^{2}$

### Ex 3

$$
A=R_{\theta}=\begin{bmatrix}\cos\theta&-\sin\theta\\ \sin\theta&\cos\theta\end{bmatrix} \text{ with } \theta\ne 0\mod\pi
$$


This means 

$$
\chi_{A}(x)=\begin{bmatrix}\cos\theta-x&-\sin\theta\\ \sin\theta&\cos\theta-x\end{bmatrix}=(\cos\theta-x)^{2}+\sin^2(\theta)
$$

Which doesn't allow diagonalization. f thus doesn't have any eigenvalues or eigenvectors.

## Special cases

If $\omega=0$, $f(v)=0v=(0,0)$

If $\omega=1$, $f(v)=v$

If $\omega=-1$, $f(v)=-v$



## Diagonalization

Same values (f and A) as the previous part.

Let's say f has 2 eigenvalues $\omega_{1}$ and $\omega_{2}$, and we choose an ordered basis $B=v_{1},v_{2}$, where the $v_{i}$ are eigenvectors associated with $\omega_{i}$.

f in that basis becomes
$$
[f]_B=\begin{bmatrix}\omega_{1}&0\\0&\omega_{2}\end{bmatrix}
$$

In this case we call B an eigenbasis for f. with each vector spanning the eigenspace $\ker(f-\omega_{i}I_{\mathbb{R}^{2}})$


So, when is a matrix diagonalizable? 

We simply need to find the discriminant of the polynomial, as follows:

$$
\Delta=(tr(A))^{2}-4\det(A)
$$

If $\Delta\lt0$, then f doesn't have any (real) eigenvalues, and no eigenvectors, meaning it can't be diagonalized.

If $\Delta=0$, then we have options: if the dimention of the kernel is 2, then $A= \omega I$. If it's 1, it's not diagonalizable. 

If, and this is the important case, $\Delta\gt0$, then $\chi_{f}(x)=(x-\omega_{1})(x-\omega_{2})$ with $\omega_{1}\ne\omega_{2}$. 
If $v_{1}$ and $v_{2}$ are eigenvectors for $\omega$ and that value (with them not being proportional because of the $\ne$).

If we make a basis $B=v_{1},v_{2}$ out of these eigenvectors, then

$$
[f]_{B}=\begin{bmatrix}\omega_{1}&0\\0&\omega_{2}\end{bmatrix}
$$

and geometrically, writing any vector as:

$$v=x_{1}v_{1}+x_{2}v_{2}$$

 we have

$$
f(v)=\omega_{1}x_{1}v_{1}+\omega_{2}x_{2}v_{2}
$$

Which is why diagonalisation is useful.

[[3. Linear application from R2 to R2|Previous Chapter]]
